<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>behrica on</title><link>https://scicloj.github.io/contributors/behrica/</link><description>Recent content in behrica on</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Sat, 10 Sep 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://scicloj.github.io/contributors/behrica/index.xml" rel="self" type="application/rss+xml"/><item><title>Predict real vs. fake disaster tweets</title><link>https://scicloj.github.io/blog/predict-real-vs.-fake-disaster-tweets/</link><pubDate>Sat, 10 Sep 2022 00:00:00 +0000</pubDate><guid>https://scicloj.github.io/blog/predict-real-vs.-fake-disaster-tweets/</guid><description>Any a little bit more serious machine learning, like participating in a Kaggle competition, requires in my view a form of ML experiment tracking.
As ML requires a lot of different trying of code, models and hyper-parameters, we need to have a tools which keeps track of this.
These type of tools can be programming language independent, as they see the code (whatever code) such as one of the assets to track (among hyper-parameters, performance metrics and data)</description></item></channel></rss>